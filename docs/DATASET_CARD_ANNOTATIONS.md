---
language:
- en
license: mit
task_categories:
- text-classification
- text-classification
tags:
- legal
- fineweb
- synthetic-data
- annotations
- llm-judgement
pretty_name: FineWeb-Legal-Annotations
size_categories:
- 10K<n<100K
configs:
  - config_name: default
    data_files:
      - split: train
        path: data/train.parquet
      - split: validation
        path: data/val.parquet
      - split: test
        path: data/test.parquet
---

# ⚖️ FineWeb-Legal-Annotations

<center>
    <img src="https://raw.githubusercontent.com/NoeFlandre/fineweb-legal/main/assets/logo.png" alt="FineWeb-Legal Logo" width="400"/>
</center>

> **57,107 high-quality annotations** of web text for legal relevance, generated by Mistral-Medium.

**Repo:** [GitHub](https://github.com/NoeFlandre/fineweb-legal) | **Derived Model:** [Legal Classifier](https://huggingface.co/NoeFlandre/fineweb-legal-classifier)

## Overview

This dataset contains **57,107** web documents from FineWeb that have been annotated with a "Legal Quality Score" (0-5) by `mistral-medium-latest`.

These annotations serve as the **ground truth** for training lightweight legal quality classifiers (like our Gemma-Embedding-300m based classifier). It enables the community to train their own filters to extract legal data from the web.

## Dataset Structure

The data is split into stratified Train/Val/Test sets to ensure fair evaluation across all quality scores.

| Split | Count | Percentage |
|-------|-------|------------|
| **Train** | 39,974 | 70% |
| **Validation** | 8,566 | 15% |
| **Test** | 8,567 | 15% |
| **Total** | **57,107** | **100%** |

### Schema

| Column | Type | Description |
|--------|------|-------------|
| `id` | string | Unique document ID (from FineWeb) |
| `text` | string | The full document text content |
| `score` | int64 | **0-5** integer score of legal quality |
| `reasoning` | string | Explanation generated by Mistral for the score |
| `url` | string | Source URL of the document |
| `text_hash` | string | SHA-256 hash for deduplication |

## Annotation Methodology

### The Judge
We used **Mistral-Medium** (specifically `mistral-medium-latest`) as the annotator. It was provided with the full text of the webpage and asked to rate it based on its utility for training a legal LLM.

### Scoring Rubric

| Score | Label | Description | Criteria |
|:-----:|:------|:------------|:---------|
| **0** | Noise/Spam | Irrelevant | Navigation menus, cookie notices, error pages, or complete gibberish. |
| **1** | Marketing | Low Quality | Law firm advertisements ("Call 1-800-LAW-NOW"), generic news summaries, or superficial content. |
| **2** | Basic Info | Layman | Public forums (Reddit/Quora), Wikipedia-style summaries, or ELI5 legal explanations. |
| **3** | Useful | Professional | Legal news (Reuters/Law360), government guides (IRS/USCIS), or detailed legal bogs. |
| **4** | High Value | Primary Source | Case law text, statutes, regulations, contracts, or court filings. |
| **5** | Gold Standard | Academic/Elite | Supreme Court opinions, Law Review articles, or highly authoritative legal treatises. |

### Source Data
The documents were sampled from the **FineWeb 10BT** subset using a high-recall heuristic pre-filter (keywords + regex) to ensure a high density of legal documents before expensive LLM annotation.

## Usage

```python
from datasets import load_dataset

dataset = load_dataset("NoeFlandre/fineweb-legal-annotations")

# Example: Filter for 'Gold Standard' documents
gold_docs = dataset.filter(lambda x: x["score"] == 5)
print(gold_docs["train"][0]["reasoning"])
```

## License

This dataset is released under the **MIT License**.
The source texts are from FineWeb (ODC-By 1.0).
